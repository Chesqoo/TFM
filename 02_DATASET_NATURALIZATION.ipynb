{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este Notebook tiene como objetivo naturalizar la columna \"docstring\" de un dataset orientado a finetuning conversacional para generación de código con LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Naturalización del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Crear la carpeta /datasets si no existe\n",
    "output_dir = './datasets'\n",
    "checkpoint_interval = 50\n",
    "checkpoint_dir = f'{output_dir}/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Cargar los DataFrames filtrados desde los archivos JSON\n",
    "df_train_filtered = pd.read_json(f'{output_dir}/train_filtered.json', orient='records', lines=True)\n",
    "df_validation_filtered = pd.read_json(f'{output_dir}/validation_filtered.json', orient='records', lines=True)\n",
    "df_test_filtered = pd.read_json(f'{output_dir}/test_filtered.json', orient='records', lines=True)\n",
    "\n",
    "# Configurar el dispositivo y cargar el pipeline con LLaMA 3 8B Instruct\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Función para transformar un docstring a una descripción contextual usando el pipeline de LLaMA 3 8B Instruct\n",
    "def transformar_docstring_con_llama(docstring, code):\n",
    "    \n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Imagine you're a user working on a project and you need to request a specific function. Describe your need in a detailed and natural way, as if you were asking a developer to implement this function for you. Do not include any code.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"I'm working on a project and I need help with the following function:\\n\\nFunction:\\n{code}\\n\\nCurrent docstring:\\n'{docstring}'\\n\\nPlease describe in a natural way what you need this function to do, including the context or scenario where it will be used.\"},\n",
    "    ]\n",
    "\n",
    "    response = text_generation_pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=250,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    # Extraer solo la respuesta del \"assistant\"\n",
    "    generated_text = \"\"\n",
    "    if isinstance(response, list) and isinstance(response[0], dict):\n",
    "        for part in response[0].get(\"generated_text\", []):\n",
    "            if part.get(\"role\") == \"assistant\" and \"content\" in part:\n",
    "                generated_text += part[\"content\"] + \" \"\n",
    "    \n",
    "    cleaned_text = generated_text.strip()\n",
    "    \n",
    "    print(\"Cleaned text:\", cleaned_text)  # Línea de depuración para ver el texto limpio generado\n",
    "    return cleaned_text\n",
    "\n",
    "# Función para procesar y guardar en checkpoints\n",
    "def procesar_y_guardar(df, df_name):\n",
    "    contextual_docstrings = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'{df_name}_contextual_checkpoint.json')\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        df_checkpoint = pd.read_json(checkpoint_path, orient='records', lines=True)\n",
    "        contextual_docstrings = df_checkpoint['docstring_contextual'].tolist()\n",
    "        start_idx = len(contextual_docstrings)\n",
    "    \n",
    "    for i, (_, row) in enumerate(tqdm(df.iterrows(), total=len(df), desc=f\"Processing {df_name} dataset\")):\n",
    "        if i < start_idx:\n",
    "            continue  # Saltar las filas ya procesadas\n",
    "        \n",
    "        contextual_docstrings.append(transformar_docstring_con_llama(row['docstring'], row['code']))\n",
    "        \n",
    "        if (i + 1) % checkpoint_interval == 0:\n",
    "            df_checkpoint = pd.DataFrame({'docstring_contextual': contextual_docstrings})\n",
    "            df_checkpoint.to_json(checkpoint_path, orient='records', lines=True)\n",
    "            print(f\"Checkpoint guardado en {checkpoint_path} hasta la fila {i + 1}.\")\n",
    "    \n",
    "    df['docstring_contextual'] = contextual_docstrings\n",
    "    df.to_json(f'{output_dir}/{df_name}_filtered_contextual.json', orient='records', lines=True)\n",
    "    print(f\"{df_name.capitalize()} dataset guardado exitosamente en {output_dir}.\")\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        os.remove(checkpoint_path)\n",
    "\n",
    "# Procesar cada dataset con checkpoints\n",
    "procesar_y_guardar(df_train_filtered, \"train\")\n",
    "procesar_y_guardar(df_validation_filtered, \"validation\")\n",
    "procesar_y_guardar(df_test_filtered, \"test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
